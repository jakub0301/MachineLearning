{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MHx_ejLVa6H2"
   },
   "source": [
    "## #Hello World - Machine Learning Recipes #1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xRDsmE57ThHH",
    "outputId": "53430930-851a-4817-a495-cc1d020bf4ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orange']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "features = [[140, 1],[130, 1], [150, 0], [170, 0]]\n",
    "labels = [[\"apple\"] , [\"apple\"] , [\"orange\"] , [\"orange\"]]\n",
    "#DecisionTreeClassifier \n",
    "clf = tree.DecisionTreeClassifier()\n",
    "#Train \n",
    "clf = clf.fit(features, labels)\n",
    "#Predict\n",
    "print (clf.predict([[150, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9zu9uah9m-B3"
   },
   "source": [
    "## Visualizing a Decision Tree - Machine Learning Recipes #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tB8NqBQm8F0"
   },
   "source": [
    "Import dataset and print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F7_OTlGDTmWZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "print (iris.feature_names) \n",
    "print (iris.target_names)\n",
    "print (iris.data[0])\n",
    "print (iris.target[0])\n",
    "for i in range(len(iris.target)):\n",
    "  print(\"Example %d: label %s, features %s\" % (i, iris.target[i], iris.data[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "96aJ5K7fe8Rk"
   },
   "source": [
    "Training a classifier and testing predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BAv6pwL5TmKv",
    "outputId": "c4ec99ed-e775-4728-d52f-6164bc71b009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "\n",
    "iris = load_iris()\n",
    "test_idx = [0,50,100]\n",
    "\n",
    "#training data\n",
    "train_target = np.delete(iris.target, test_idx)\n",
    "train_data = np.delete(iris.data, test_idx, axis=0)\n",
    "\n",
    "#testing data \n",
    "test_target = iris.target[test_idx]\n",
    "test_data = iris.data[test_idx]\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(train_data, train_target)\n",
    "\n",
    "print (test_target)\n",
    "print (clf.predict(test_data))\n",
    "\n",
    "\n",
    "#visualization\n",
    "from sklearn.externals.six import StringIO\n",
    "import pydot\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(clf, out_file=dot_data, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, impurity=False)\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "graph[0].write_pdf(\"iris.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6MSlUgKQYrS4"
   },
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "bW6F_hYSTmdX",
    "outputId": "611e4ca5-d992-4215-a416-b3c718dfff67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris  iris.pdf\tsample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "J4OZ1dXETmgI",
    "outputId": "f7adec2e-c0e9-4a45-bc40-4d27af911ec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.1 3.5 1.4 0.2] 0\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print (test_data[0], test_target[0])\n",
    "\n",
    "print (iris.feature_names, iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7q2yswVRTqt"
   },
   "source": [
    "## What Makes a Good Feature? - Machine Learning Recipes #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "N5cwyzG4Tmiw",
    "outputId": "99aa806c-ee0e-4b55-d90e-8a1886fb26e7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeYAAAFOCAYAAABT3L5MAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEWYAKIgwA0BB/h8CY03IC/9CHAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "greyhounds = 500\n",
    "labs = 500\n",
    "\n",
    "grey_height = 28 + 4 * np.random.randn(greyhounds)\n",
    "lab_height = 24 + 4 * np.random.randn(labs)\n",
    "\n",
    "plt.hist([grey_height, lab_height], stacked=True, color=['r', 'b'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EH1pHBlWwlRF"
   },
   "source": [
    "## Let’s Write a Pipeline - Machine Learning Recipes #4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "aivLlKZJDNOy",
    "outputId": "02b2863e-9a0a-47c3-c2a5-12a409b7b02a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 0 0 2 1 2 2 2 2 2 2 2 2 2 2 0 1 2 1 2 0 0 1 1 2 2 0 0 0 1 2 0 0 2 2 1\n",
      " 0 1 1 0 1 0 2 0 1 2 0 0 2 0 1 1 1 1 2 0 0 2 2 1 0 0 1 1 0 1 0 1 0 2 1 2 0\n",
      " 0]\n",
      "0.96\n"
     ]
    }
   ],
   "source": [
    "#import a dataset\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test =train_test_split(X, Y, test_size = .5)\n",
    "\n",
    "from sklearn import tree \n",
    "my_classifier = tree.DecisionTreeClassifier()\n",
    "\n",
    "my_classifier.fit(X_train, Y_train)\n",
    "\n",
    "predictions = my_classifier.predict(X_test)\n",
    "print (predictions)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (accuracy_score(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XCLc13x6sRHn"
   },
   "source": [
    "*KNeighboursClassifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "KBrcxAxbsRcQ",
    "outputId": "8325f7e8-3b97-4f86-bc44-b3a823ea4eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 1 2 1 1 1 2 2 0 0 1 0 0 0 0 2 2 0 1 0 1 0 0 2 0 1 2 1 2 1 1 2 0 0 1\n",
      " 1 1 0 2 0 0 2 2 0 2 0 1 0 2 1 2 0 2 1 1 0 1 1 0 2 2 1 2 2 2 1 2 1 0 1 1 0\n",
      " 1]\n",
      "0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "#import a dataset\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test =train_test_split(X, Y, test_size = .5)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "my_classifier = KNeighborsClassifier()\n",
    "\n",
    "my_classifier.fit(X_train, Y_train)\n",
    "\n",
    "predictions = my_classifier.predict(X_test)\n",
    "print (predictions)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (accuracy_score(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PURCWJxP92ze"
   },
   "source": [
    "## Writing Our First Classifier - Machine Learning Recipes #5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "ATu0fQ3A95lV",
    "outputId": "77a1aefb-e7f9-44e3-f137-82f4bbc3e817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 2, 1, 1, 1, 0, 1, 1, 2, 0, 0, 2, 2, 1, 0, 1, 1, 0, 1, 0, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 0, 1, 1, 0, 0, 1, 1, 2, 1, 1, 2, 2, 0, 0, 0, 2, 1, 1, 1, 2, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 2, 1, 0, 1, 0, 2, 0, 0]\n",
      "0.28\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class ScrappyKNN():\n",
    "  def fit(self, X_train, Y_train):\n",
    "    self.X_train = X_train\n",
    "    self.Y_train = Y_train\n",
    "  \n",
    "  def predict(self, X_test):\n",
    "    predictions = []\n",
    "    for row in X_test:\n",
    "      label = random.choice(self.Y_train)\n",
    "      predictions.append(label)\n",
    "    return predictions\n",
    "  \n",
    "  \n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test =train_test_split(X, Y, test_size = .5)\n",
    "\n",
    "my_classifier = ScrappyKNN()\n",
    "\n",
    "my_classifier.fit(X_train, Y_train)\n",
    "\n",
    "predictions = my_classifier.predict(X_test)\n",
    "print (predictions)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (accuracy_score(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "hth3dP-pBfWp",
    "outputId": "c101ca5c-bdd7-4d1c-f5ba-acf1bec7f640"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "0.32\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def euc(a,b):\n",
    "  return distance.euclidean(a,b)\n",
    "\n",
    "class ScrappyKNN():\n",
    "  def fit(self, X_train, Y_train):\n",
    "    self.X_train = X_train\n",
    "    self.Y_train = Y_train\n",
    "  \n",
    "  def predict(self, X_test):\n",
    "    predictions = []\n",
    "    for row in X_test:\n",
    "      label = self.closest(row)\n",
    "      predictions.append(label)\n",
    "    return predictions\n",
    "  \n",
    "  def closest(self, row):\n",
    "    best_dist = euc(row, self.X_train[0])\n",
    "    best_index = 0\n",
    "    for i in range(1, len(self.X_train)):\n",
    "      dist = euc(row, self.X_train[i])\n",
    "      if dist < best_dist:\n",
    "        best_dist = dist\n",
    "        best_index = i\n",
    "      return self.Y_train[best_index]\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test =train_test_split(X, Y, test_size = .5)\n",
    "\n",
    "my_classifier = ScrappyKNN()\n",
    "\n",
    "my_classifier.fit(X_train, Y_train)\n",
    "\n",
    "predictions = my_classifier.predict(X_test)\n",
    "print (predictions)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (accuracy_score(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QUdl5BgEDaCn"
   },
   "source": [
    "## Train an Image Classifier with TensorFlow for Poets - Machine Learning Recipes #6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VDcfap85sBtB"
   },
   "source": [
    "## Classifying Handwritten Digits with TF.Learn - Machine Learning Recipes #7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jE1P1SmKsPyp"
   },
   "source": [
    "[LINK](https://github.com/random-forests/tutorials/blob/master/ep7.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VwJNNyxqsT6x"
   },
   "source": [
    "## Let’s Write a Decision Tree Classifier from Scratch - Machine Learning Recipes #8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "aCSv_ocYuH-1",
    "outputId": "8887999c-e21c-4344-e87d-f0ab1ffdadcd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is diameter >= 3?\n",
      "--> True:\n",
      "  Is color == Yellow?\n",
      "  --> True:\n",
      "    Predict {'Apple': 1, 'Lemon': 1}\n",
      "  --> False:\n",
      "    Predict {'Apple': 1}\n",
      "--> False:\n",
      "  Predict {'Grape': 2}\n",
      "Actual: Apple. Predicted: {'Apple': '100%'}\n",
      "Actual: Apple. Predicted: {'Apple': '50%', 'Lemon': '50%'}\n",
      "Actual: Grape. Predicted: {'Grape': '100%'}\n",
      "Actual: Grape. Predicted: {'Grape': '100%'}\n",
      "Actual: Lemon. Predicted: {'Apple': '50%', 'Lemon': '50%'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Code to accompany Machine Learning Recipes #8.\n",
    "\n",
    "We'll write a Decision Tree Classifier, in pure Python.\n",
    "\"\"\"\n",
    "\n",
    "# For Python 2 / 3 compatability\n",
    "from __future__ import print_function\n",
    "\n",
    "# Toy dataset.\n",
    "# Format: each row is an example.\n",
    "# The last column is the label.\n",
    "# The first two columns are features.\n",
    "# Feel free to play with it by adding more features & examples.\n",
    "# Interesting note: I've written this so the 2nd and 5th examples\n",
    "# have the same features, but different labels - so we can see how the\n",
    "# tree handles this case.\n",
    "training_data = [\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Yellow', 3, 'Apple'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "]\n",
    "\n",
    "# Column labels.\n",
    "# These are used only to print the tree.\n",
    "header = [\"color\", \"diameter\", \"label\"]\n",
    "\n",
    "\n",
    "def unique_vals(rows, col):\n",
    "    \"\"\"Find the unique values for a column in a dataset.\"\"\"\n",
    "    return set([row[col] for row in rows])\n",
    "\n",
    "#######\n",
    "# Demo:\n",
    "# unique_vals(training_data, 0)\n",
    "# unique_vals(training_data, 1)\n",
    "#######\n",
    "\n",
    "\n",
    "def class_counts(rows):\n",
    "    \"\"\"Counts the number of each type of example in a dataset.\"\"\"\n",
    "    counts = {}  # a dictionary of label -> count.\n",
    "    for row in rows:\n",
    "        # in our dataset format, the label is always the last column\n",
    "        label = row[-1]\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        counts[label] += 1\n",
    "    return counts\n",
    "\n",
    "#######\n",
    "# Demo:\n",
    "# class_counts(training_data)\n",
    "#######\n",
    "\n",
    "\n",
    "def is_numeric(value):\n",
    "    \"\"\"Test if a value is numeric.\"\"\"\n",
    "    return isinstance(value, int) or isinstance(value, float)\n",
    "\n",
    "#######\n",
    "# Demo:\n",
    "# is_numeric(7)\n",
    "# is_numeric(\"Red\")\n",
    "#######\n",
    "\n",
    "\n",
    "class Question:\n",
    "    \"\"\"A Question is used to partition a dataset.\n",
    "\n",
    "    This class just records a 'column number' (e.g., 0 for Color) and a\n",
    "    'column value' (e.g., Green). The 'match' method is used to compare\n",
    "    the feature value in an example to the feature value stored in the\n",
    "    question. See the demo below.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column, value):\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "\n",
    "    def match(self, example):\n",
    "        # Compare the feature value in an example to the\n",
    "        # feature value in this question.\n",
    "        val = example[self.column]\n",
    "        if is_numeric(val):\n",
    "            return val >= self.value\n",
    "        else:\n",
    "            return val == self.value\n",
    "\n",
    "    def __repr__(self):\n",
    "        # This is just a helper method to print\n",
    "        # the question in a readable format.\n",
    "        condition = \"==\"\n",
    "        if is_numeric(self.value):\n",
    "            condition = \">=\"\n",
    "        return \"Is %s %s %s?\" % (\n",
    "            header[self.column], condition, str(self.value))\n",
    "\n",
    "#######\n",
    "# Demo:\n",
    "# Let's write a question for a numeric attribute\n",
    "# Question(1, 3)\n",
    "# How about one for a categorical attribute\n",
    "# q = Question(0, 'Green')\n",
    "# Let's pick an example from the training set...\n",
    "# example = training_data[0]\n",
    "# ... and see if it matches the question\n",
    "# q.match(example)\n",
    "#######\n",
    "\n",
    "\n",
    "def partition(rows, question):\n",
    "    \"\"\"Partitions a dataset.\n",
    "\n",
    "    For each row in the dataset, check if it matches the question. If\n",
    "    so, add it to 'true rows', otherwise, add it to 'false rows'.\n",
    "    \"\"\"\n",
    "    true_rows, false_rows = [], []\n",
    "    for row in rows:\n",
    "        if question.match(row):\n",
    "            true_rows.append(row)\n",
    "        else:\n",
    "            false_rows.append(row)\n",
    "    return true_rows, false_rows\n",
    "\n",
    "\n",
    "#######\n",
    "# Demo:\n",
    "# Let's partition the training data based on whether rows are Red.\n",
    "# true_rows, false_rows = partition(training_data, Question(0, 'Red'))\n",
    "# This will contain all the 'Red' rows.\n",
    "# true_rows\n",
    "# This will contain everything else.\n",
    "# false_rows\n",
    "#######\n",
    "\n",
    "def gini(rows):\n",
    "    \"\"\"Calculate the Gini Impurity for a list of rows.\n",
    "\n",
    "    There are a few different ways to do this, I thought this one was\n",
    "    the most concise. See:\n",
    "    https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity\n",
    "    \"\"\"\n",
    "    counts = class_counts(rows)\n",
    "    impurity = 1\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        impurity -= prob_of_lbl**2\n",
    "    return impurity\n",
    "\n",
    "\n",
    "#######\n",
    "# Demo:\n",
    "# Let's look at some example to understand how Gini Impurity works.\n",
    "#\n",
    "# First, we'll look at a dataset with no mixing.\n",
    "# no_mixing = [['Apple'],\n",
    "#              ['Apple']]\n",
    "# this will return 0\n",
    "# gini(no_mixing)\n",
    "#\n",
    "# Now, we'll look at dataset with a 50:50 apples:oranges ratio\n",
    "# some_mixing = [['Apple'],\n",
    "#               ['Orange']]\n",
    "# this will return 0.5 - meaning, there's a 50% chance of misclassifying\n",
    "# a random example we draw from the dataset.\n",
    "# gini(some_mixing)\n",
    "#\n",
    "# Now, we'll look at a dataset with many different labels\n",
    "# lots_of_mixing = [['Apple'],\n",
    "#                  ['Orange'],\n",
    "#                  ['Grape'],\n",
    "#                  ['Grapefruit'],\n",
    "#                  ['Blueberry']]\n",
    "# This will return 0.8\n",
    "# gini(lots_of_mixing)\n",
    "#######\n",
    "\n",
    "def info_gain(left, right, current_uncertainty):\n",
    "    \"\"\"Information Gain.\n",
    "\n",
    "    The uncertainty of the starting node, minus the weighted impurity of\n",
    "    two child nodes.\n",
    "    \"\"\"\n",
    "    p = float(len(left)) / (len(left) + len(right))\n",
    "    return current_uncertainty - p * gini(left) - (1 - p) * gini(right)\n",
    "\n",
    "#######\n",
    "# Demo:\n",
    "# Calculate the uncertainy of our training data.\n",
    "# current_uncertainty = gini(training_data)\n",
    "#\n",
    "# How much information do we gain by partioning on 'Green'?\n",
    "# true_rows, false_rows = partition(training_data, Question(0, 'Green'))\n",
    "# info_gain(true_rows, false_rows, current_uncertainty)\n",
    "#\n",
    "# What about if we partioned on 'Red' instead?\n",
    "# true_rows, false_rows = partition(training_data, Question(0,'Red'))\n",
    "# info_gain(true_rows, false_rows, current_uncertainty)\n",
    "#\n",
    "# It looks like we learned more using 'Red' (0.37), than 'Green' (0.14).\n",
    "# Why? Look at the different splits that result, and see which one\n",
    "# looks more 'unmixed' to you.\n",
    "# true_rows, false_rows = partition(training_data, Question(0,'Red'))\n",
    "#\n",
    "# Here, the true_rows contain only 'Grapes'.\n",
    "# true_rows\n",
    "#\n",
    "# And the false rows contain two types of fruit. Not too bad.\n",
    "# false_rows\n",
    "#\n",
    "# On the other hand, partitioning by Green doesn't help so much.\n",
    "# true_rows, false_rows = partition(training_data, Question(0,'Green'))\n",
    "#\n",
    "# We've isolated one apple in the true rows.\n",
    "# true_rows\n",
    "#\n",
    "# But, the false-rows are badly mixed up.\n",
    "# false_rows\n",
    "#######\n",
    "\n",
    "\n",
    "def find_best_split(rows):\n",
    "    \"\"\"Find the best question to ask by iterating over every feature / value\n",
    "    and calculating the information gain.\"\"\"\n",
    "    best_gain = 0  # keep track of the best information gain\n",
    "    best_question = None  # keep train of the feature / value that produced it\n",
    "    current_uncertainty = gini(rows)\n",
    "    n_features = len(rows[0]) - 1  # number of columns\n",
    "\n",
    "    for col in range(n_features):  # for each feature\n",
    "\n",
    "        values = set([row[col] for row in rows])  # unique values in the column\n",
    "\n",
    "        for val in values:  # for each value\n",
    "\n",
    "            question = Question(col, val)\n",
    "\n",
    "            # try splitting the dataset\n",
    "            true_rows, false_rows = partition(rows, question)\n",
    "\n",
    "            # Skip this split if it doesn't divide the\n",
    "            # dataset.\n",
    "            if len(true_rows) == 0 or len(false_rows) == 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate the information gain from this split\n",
    "            gain = info_gain(true_rows, false_rows, current_uncertainty)\n",
    "\n",
    "            # You actually can use '>' instead of '>=' here\n",
    "            # but I wanted the tree to look a certain way for our\n",
    "            # toy dataset.\n",
    "            if gain >= best_gain:\n",
    "                best_gain, best_question = gain, question\n",
    "\n",
    "    return best_gain, best_question\n",
    "\n",
    "#######\n",
    "# Demo:\n",
    "# Find the best question to ask first for our toy dataset.\n",
    "# best_gain, best_question = find_best_split(training_data)\n",
    "# FYI: is color == Red is just as good. See the note in the code above\n",
    "# where I used '>='.\n",
    "#######\n",
    "\n",
    "class Leaf:\n",
    "    \"\"\"A Leaf node classifies data.\n",
    "\n",
    "    This holds a dictionary of class (e.g., \"Apple\") -> number of times\n",
    "    it appears in the rows from the training data that reach this leaf.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rows):\n",
    "        self.predictions = class_counts(rows)\n",
    "\n",
    "\n",
    "class Decision_Node:\n",
    "    \"\"\"A Decision Node asks a question.\n",
    "\n",
    "    This holds a reference to the question, and to the two child nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 question,\n",
    "                 true_branch,\n",
    "                 false_branch):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n",
    "\n",
    "\n",
    "def build_tree(rows):\n",
    "    \"\"\"Builds the tree.\n",
    "\n",
    "    Rules of recursion: 1) Believe that it works. 2) Start by checking\n",
    "    for the base case (no further information gain). 3) Prepare for\n",
    "    giant stack traces.\n",
    "    \"\"\"\n",
    "\n",
    "    # Try partitioing the dataset on each of the unique attribute,\n",
    "    # calculate the information gain,\n",
    "    # and return the question that produces the highest gain.\n",
    "    gain, question = find_best_split(rows)\n",
    "\n",
    "    # Base case: no further info gain\n",
    "    # Since we can ask no further questions,\n",
    "    # we'll return a leaf.\n",
    "    if gain == 0:\n",
    "        return Leaf(rows)\n",
    "\n",
    "    # If we reach here, we have found a useful feature / value\n",
    "    # to partition on.\n",
    "    true_rows, false_rows = partition(rows, question)\n",
    "\n",
    "    # Recursively build the true branch.\n",
    "    true_branch = build_tree(true_rows)\n",
    "\n",
    "    # Recursively build the false branch.\n",
    "    false_branch = build_tree(false_rows)\n",
    "\n",
    "    # Return a Question node.\n",
    "    # This records the best feature / value to ask at this point,\n",
    "    # as well as the branches to follow\n",
    "    # dependingo on the answer.\n",
    "    return Decision_Node(question, true_branch, false_branch)\n",
    "\n",
    "\n",
    "def print_tree(node, spacing=\"\"):\n",
    "    \"\"\"World's most elegant tree printing function.\"\"\"\n",
    "\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf):\n",
    "        print (spacing + \"Predict\", node.predictions)\n",
    "        return\n",
    "\n",
    "    # Print the question at this node\n",
    "    print (spacing + str(node.question))\n",
    "\n",
    "    # Call this function recursively on the true branch\n",
    "    print (spacing + '--> True:')\n",
    "    print_tree(node.true_branch, spacing + \"  \")\n",
    "\n",
    "    # Call this function recursively on the false branch\n",
    "    print (spacing + '--> False:')\n",
    "    print_tree(node.false_branch, spacing + \"  \")\n",
    "\n",
    "\n",
    "def classify(row, node):\n",
    "    \"\"\"See the 'rules of recursion' above.\"\"\"\n",
    "\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf):\n",
    "        return node.predictions\n",
    "\n",
    "    # Decide whether to follow the true-branch or the false-branch.\n",
    "    # Compare the feature / value stored in the node,\n",
    "    # to the example we're considering.\n",
    "    if node.question.match(row):\n",
    "        return classify(row, node.true_branch)\n",
    "    else:\n",
    "        return classify(row, node.false_branch)\n",
    "\n",
    "\n",
    "#######\n",
    "# Demo:\n",
    "# The tree predicts the 1st row of our\n",
    "# training data is an apple with confidence 1.\n",
    "# my_tree = build_tree(training_data)\n",
    "# classify(training_data[0], my_tree)\n",
    "#######\n",
    "\n",
    "def print_leaf(counts):\n",
    "    \"\"\"A nicer way to print the predictions at a leaf.\"\"\"\n",
    "    total = sum(counts.values()) * 1.0\n",
    "    probs = {}\n",
    "    for lbl in counts.keys():\n",
    "        probs[lbl] = str(int(counts[lbl] / total * 100)) + \"%\"\n",
    "    return probs\n",
    "\n",
    "\n",
    "#######\n",
    "# Demo:\n",
    "# Printing that a bit nicer\n",
    "# print_leaf(classify(training_data[0], my_tree))\n",
    "#######\n",
    "\n",
    "#######\n",
    "# Demo:\n",
    "# On the second example, the confidence is lower\n",
    "# print_leaf(classify(training_data[1], my_tree))\n",
    "#######\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    my_tree = build_tree(training_data)\n",
    "\n",
    "    print_tree(my_tree)\n",
    "\n",
    "    # Evaluate\n",
    "    testing_data = [\n",
    "        ['Green', 3, 'Apple'],\n",
    "        ['Yellow', 4, 'Apple'],\n",
    "        ['Red', 2, 'Grape'],\n",
    "        ['Red', 1, 'Grape'],\n",
    "        ['Yellow', 3, 'Lemon'],\n",
    "    ]\n",
    "\n",
    "    for row in testing_data:\n",
    "        print (\"Actual: %s. Predicted: %s\" %\n",
    "               (row[-1], print_leaf(classify(row, my_tree))))\n",
    "\n",
    "# Next steps\n",
    "# - add support for missing (or unseen) attributes\n",
    "# - prune the tree to prevent overfitting\n",
    "# - add support for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MHx_ejLVa6H2"
   },
   "source": [
    "## #Learn TensorFlow 1: The \"Hello World\" of machine learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-2.0, 1.0, 4.0, 7.0, 10.0, 13.0], dtype=float)\n",
    "\n",
    "model.fit(xs, ys, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31.000034]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([10.0]))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
